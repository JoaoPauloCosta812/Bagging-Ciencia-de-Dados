{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6753db0-f2b6-4605-ac73-f67be239fd1d",
   "metadata": {},
   "source": [
    "# Modulo 23 Atividade 02\n",
    "#### João Paulo Costa\n",
    "\n",
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947bb62-eb09-4269-aca7-ee279a5dd2cd",
   "metadata": {},
   "source": [
    "#### 1) Passo a passo do algoritmo Random Forest\n",
    "\n",
    "O Random Forest (ou Floresta Aleatória) é uma extensão do Bagging, aplicada a árvores de decisão.\n",
    "Ele adiciona um passo extra de aleatoriedade na escolha dos atributos, o que reduz ainda mais a correlação entre as árvores.\n",
    "\n",
    "Passos:\n",
    "\n",
    "  1. Bootstrap (amostragem com reposição):\n",
    "\n",
    "   * Cria várias amostras aleatórias do conjunto de treino, com reposição (como no Bagging).\n",
    "\n",
    "  2. Seleção aleatória de variáveis (feature selection):\n",
    "\n",
    "   * Em cada divisão (nó) de uma árvore, em vez de considerar todas as variáveis preditoras, o algoritmo escolhe um subconjunto aleatório de variáveis.\n",
    "\n",
    "   * Isso garante que as árvores fiquem mais “diversas”.\n",
    "\n",
    "  3. Treinamento de várias árvores (modelagem):\n",
    "\n",
    "   * Cada árvore é treinada em sua amostra bootstrap e usa apenas parte das variáveis disponíveis em cada divisão.\n",
    "\n",
    "  4. Agregação das previsões:\n",
    "\n",
    "   * Para classificação → voto da maioria das árvores.\n",
    "\n",
    "   * Para regressão → média das previsões das árvores.\n",
    "\n",
    "  5. Resultado final:\n",
    "\n",
    "   * Um modelo mais robusto, com baixo viés e baixa variância."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8431f7b4-6812-42a6-bb4e-041b5e0c81e1",
   "metadata": {},
   "source": [
    "#### 2. Explicação com suas palavras\n",
    "\n",
    "O Random Forest é como um bagging aprimorado, assim como no mesmo, fortalecemos um modelo de aprendizado treinando várias versões diferentes dele em subconjuntos diferentes do mesmo conjunto de dados, mas ao inves de um modelo simples utilizamos arvores de decisões, uma para cada subconjunto, e possuimos uma aleatoriedade e independencia maior graças ao Feature Selection aplicados apra criação dos subconjuntos. Assim, temos no fim a combinação de várias árvores simples para formar um modelo muito mais estável e com menor chance de overfitting do que uma árvore única ou um bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f0cab-7db9-4a60-bd50-07fac7a8cd30",
   "metadata": {},
   "source": [
    "#### 3. Diferença entre Bagging e Random Forest\n",
    "\n",
    "| Característica            | Bagging                               | Random Forest                                                        |\n",
    "| ------------------------- | ------------------------------------- | -------------------------------------------------------------------- |\n",
    "| Tipo de modelo base       | Qualquer (ex: árvore, regressão, SVM) | Árvores de decisão                                                   |\n",
    "| Seleção de variáveis      | Usa todas as variáveis disponíveis    | Seleciona aleatoriamente um subconjunto de variáveis em cada divisão |\n",
    "| Diversidade entre modelos | Vem apenas do *bootstrap*             | Vem do *bootstrap* + *seleção aleatória de features*                 |\n",
    "| Correlação entre modelos  | Maior                                 | Menor (mais independente)                                            |\n",
    "| Desempenho                | Bom                                   | Melhor (geralmente)                                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8622af-65a4-4e40-b72a-11900dc40619",
   "metadata": {},
   "source": [
    "#### 4. Implementação em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e6803c-9544-4c6b-a874-346d7cfed0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia final com Random Forest (manual): 0.993\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "\n",
    "# 1. Carregar base de dados\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "\n",
    "# 2. Parâmetros\n",
    "n_trees = 10           # número de árvores\n",
    "n_samples = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "max_features = int(np.sqrt(n_features))  # regra comum: sqrt(p)\n",
    "models = []\n",
    "features_idx = []\n",
    "\n",
    "\n",
    "# 3. Treinamento (Bootstrap + Feature Selection + Modelagem)\n",
    "for i in range(n_trees):\n",
    "    # Bootstrap (amostra aleatória com reposição)\n",
    "    idx = np.random.choice(range(n_samples), size=n_samples, replace=True)\n",
    "    X_boot, y_boot = X[idx], y[idx]\n",
    "\n",
    "    # Seleção aleatória de features\n",
    "    feat_idx = np.random.choice(range(n_features), size=max_features, replace=False)\n",
    "    features_idx.append(feat_idx)\n",
    "\n",
    "    # Treinar árvore usando apenas as features selecionadas\n",
    "    model = DecisionTreeClassifier(random_state=i)\n",
    "    model.fit(X_boot[:, feat_idx], y_boot)\n",
    "    models.append(model)\n",
    "\n",
    "\n",
    "# 4. Agregação das previsões\n",
    "preds = []\n",
    "for model, feat_idx in zip(models, features_idx):\n",
    "    preds.append(model.predict(X[:, feat_idx]))\n",
    "\n",
    "preds = np.array(preds)\n",
    "y_pred_final = mode(preds, axis=0, keepdims=True).mode[0]\n",
    "y_pred_final = np.ravel(y_pred_final)\n",
    "\n",
    "\n",
    "# 5. Avaliação\n",
    "acc = accuracy_score(y, y_pred_final)\n",
    "print(f\"Acurácia final com Random Forest (manual): {acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
